import { LLMOrchestrator } from '../src/core/llm/LLMOrchestrator';
import { LLMProvider, LLMRequest, LLMResponse, LLMProviderConfig, LLMCapabilities } from '../src/core/llm/LLMProvider';

// Mock the dependencies
jest.mock('../src/core/promptEngineer');
jest.mock('../src/core/responseParser');
jest.mock('../src/core/tokenManager');
jest.mock('../src/core/rateLimiter');
jest.mock('../src/utils/logger', () => ({
  agentLogger: {
    child: jest.fn(() => ({
      info: jest.fn(),
      warn: jest.fn(),
      error: jest.fn(),
      debug: jest.fn()
    }))
  }
}));

// Mock LLM Provider implementation
class MockLLMProvider extends LLMProvider {
  private shouldFail = false;
  private responseDelay = 0;

  constructor(private name: string = 'mock-provider') {
    super({
      apiKey: 'test-key',
      model: 'test-model',
      temperature: 0.7,
      maxTokens: 1000
    }, { child: () => ({ info: jest.fn(), error: jest.fn() }) });
  }

  getProviderName(): string {
    return this.name;
  }

  getCapabilities(): LLMCapabilities {
    return {
      maxTokens: 4096,
      supportedModels: ['test-model'],
      supportsStreaming: false,
      supportsVision: false,
      supportsTools: false,
      rateLimit: {
        requestsPerMinute: 60,
        tokensPerMinute: 100000
      }
    };
  }

  async generateResponse(request: LLMRequest): Promise<LLMResponse> {
    if (this.shouldFail) {
      throw new Error('Mock provider error');
    }

    // Simulate delay if specified
    if (this.responseDelay > 0) {
      await new Promise(resolve => setTimeout(resolve, this.responseDelay));
    }

    return {
      content: `Mock response to: ${request.messages[request.messages.length - 1].content}`,
      tokensUsed: {
        prompt: 100,
        completion: 50,
        total: 150
      },
      model: 'mock-model',
      finishReason: 'stop'
    };
  }

  estimateTokens(text: string): number {
    return Math.ceil(text.length / 4);
  }

  calculateCost(tokensUsed: any): number {
    return tokensUsed.totalTokens * 0.002;
  }

  async healthCheck(): Promise<boolean> {
    return !this.shouldFail;
  }

  destroy(): void {
    // Mock cleanup
  }

  async getSystemStatus() {
    return {
      isHealthy: !this.shouldFail,
      rateLimitStatus: { canProceed: true },
      tokenBudgetStatus: { canProceed: true }
    };
  }

  destroy(): void {
    // Mock cleanup
  }

  // Test utilities
  setShouldFail(fail: boolean) {
    this.shouldFail = fail;
  }

  setResponseDelay(delay: number) {
    this.responseDelay = delay;
  }
}

describe('LLMOrchestrator', () => {
  let orchestrator: LLMOrchestrator;
  let mockProvider: MockLLMProvider;

  beforeEach(() => {
    mockProvider = new MockLLMProvider();
    orchestrator = new LLMOrchestrator(mockProvider);
  });

  describe('initialization', () => {
    it('should initialize with LLM provider', () => {
      expect(orchestrator).toBeDefined();
    });

    it('should get provider name', () => {
      const name = orchestrator.getProviderName();
      expect(name).toBe('mock-provider');
    });
  });

  describe('processStructuredRequest', () => {
    it('should process basic request successfully', async () => {
      const request = {
        userRequest: 'Create a new file',
        context: {
          projectContext: 'Node.js project'
        }
      };

      const result = await orchestrator.processStructuredRequest(request);

      expect(result).toBeDefined();
      expect(result.response).toBeDefined();
      expect(result.response.content).toContain('Create a new file');
      expect(result.tokenUsage).toBeDefined();
      expect(result.tokenUsage.totalTokens).toBe(150);
    });

    it('should handle requests without context', async () => {
      const request = {
        userRequest: 'Help me with this task'
      };

      const result = await orchestrator.processStructuredRequest(request);

      expect(result).toBeDefined();
      expect(result.response.content).toContain('Help me with this task');
    });

    it('should include conversation history in context', async () => {
      const request = {
        userRequest: 'Continue the previous task',
        context: {
          conversationHistory: ['Previous message 1', 'Previous message 2']
        }
      };

      const result = await orchestrator.processStructuredRequest(request);

      expect(result).toBeDefined();
      expect(result.response.content).toContain('Continue the previous task');
    });

    it('should handle provider errors gracefully', async () => {
      mockProvider.setShouldFail(true);

      const request = {
        userRequest: 'This should fail'
      };

      await expect(orchestrator.processStructuredRequest(request))
        .rejects.toThrow('Mock provider error');
    });

    it('should pass through token usage information', async () => {
      const request = {
        userRequest: 'Count my tokens'
      };

      const result = await orchestrator.processStructuredRequest(request);

      expect(result.tokenUsage.promptTokens).toBe(100);
      expect(result.tokenUsage.completionTokens).toBe(50);
      expect(result.tokenUsage.totalTokens).toBe(150);
    });

    it('should handle long running requests', async () => {
      mockProvider.setResponseDelay(100); // 100ms delay

      const request = {
        userRequest: 'This takes time to process'
      };

      const startTime = Date.now();
      const result = await orchestrator.processStructuredRequest(request);
      const endTime = Date.now();

      expect(result).toBeDefined();
      expect(endTime - startTime).toBeGreaterThanOrEqual(100);
    }, 1000);
  });

  describe('getSystemStatus', () => {
    it('should return system status from provider', async () => {
      const status = await orchestrator.getSystemStatus();

      expect(status).toBeDefined();
      expect(status.isHealthy).toBe(true);
      expect(status.rateLimitStatus).toBeDefined();
      expect(status.tokenBudgetStatus).toBeDefined();
    });

    it('should reflect unhealthy status when provider fails', async () => {
      mockProvider.setShouldFail(true);

      const status = await orchestrator.getSystemStatus();

      expect(status.isHealthy).toBe(false);
    });
  });

  describe('provider management', () => {
    it('should allow provider switching', () => {
      const newProvider = new MockLLMProvider('new-provider');
      const newOrchestrator = new LLMOrchestrator(newProvider);

      expect(newOrchestrator.getProviderName()).toBe('new-provider');
    });

    it('should clean up resources on destroy', () => {
      const destroySpy = jest.spyOn(mockProvider, 'destroy');
      
      orchestrator.destroy();

      expect(destroySpy).toHaveBeenCalled();
    });
  });

  describe('error handling and resilience', () => {
    it('should handle malformed provider responses', async () => {
      // Create a provider that returns malformed data
      class BadProvider implements LLMProvider {
        getProviderName(): string {
          return 'bad-provider';
        }

        async generateResponse(request: LLMRequest): Promise<LLMResponse> {
          return {
            content: '', // Empty content
            tokensUsed: {
              promptTokens: -1, // Invalid token count
              completionTokens: -1,
              totalTokens: -1
            },
            model: '',
            finishReason: 'error'
          };
        }

        async getSystemStatus() {
          return {
            isHealthy: true,
            rateLimitStatus: { canProceed: true },
            tokenBudgetStatus: { canProceed: true }
          };
        }

        destroy(): void {}
      }

      const badOrchestrator = new LLMOrchestrator(new BadProvider());
      
      const request = {
        userRequest: 'Test bad response'
      };

      const result = await badOrchestrator.processStructuredRequest(request);

      // Should still return a result, even if malformed
      expect(result).toBeDefined();
      expect(result.response.content).toBe('');
    });

    it('should handle network timeouts gracefully', async () => {
      class TimeoutProvider implements LLMProvider {
        getProviderName(): string {
          return 'timeout-provider';
        }

        async generateResponse(request: LLMRequest): Promise<LLMResponse> {
          // Simulate a timeout
          await new Promise((_, reject) => {
            setTimeout(() => reject(new Error('Request timeout')), 50);
          });
          
          throw new Error('Should not reach here');
        }

        async getSystemStatus() {
          return {
            isHealthy: false,
            rateLimitStatus: { canProceed: false },
            tokenBudgetStatus: { canProceed: true }
          };
        }

        destroy(): void {}
      }

      const timeoutOrchestrator = new LLMOrchestrator(new TimeoutProvider());
      
      const request = {
        userRequest: 'This will timeout'
      };

      await expect(timeoutOrchestrator.processStructuredRequest(request))
        .rejects.toThrow('Request timeout');
    });
  });

  describe('context handling', () => {
    it('should handle complex project context', async () => {
      const request = {
        userRequest: 'Analyze this codebase',
        context: {
          projectContext: 'Large TypeScript project with multiple microservices',
          conversationHistory: [
            'Previously discussed architecture',
            'Identified performance bottlenecks'
          ]
        }
      };

      const result = await orchestrator.processStructuredRequest(request);

      expect(result).toBeDefined();
      expect(result.response.content).toContain('Analyze this codebase');
    });

    it('should handle empty context gracefully', async () => {
      const request = {
        userRequest: 'Simple request',
        context: {}
      };

      const result = await orchestrator.processStructuredRequest(request);

      expect(result).toBeDefined();
      expect(result.response.content).toContain('Simple request');
    });

    it('should handle null/undefined context', async () => {
      const request = {
        userRequest: 'Request without context'
      };

      const result = await orchestrator.processStructuredRequest(request);

      expect(result).toBeDefined();
      expect(result.response.content).toContain('Request without context');
    });
  });

  describe('performance and scalability', () => {
    it('should handle multiple concurrent requests', async () => {
      const requests = Array.from({ length: 5 }, (_, i) => ({
        userRequest: `Concurrent request ${i + 1}`
      }));

      const promises = requests.map(request => 
        orchestrator.processStructuredRequest(request)
      );

      const results = await Promise.all(promises);

      expect(results).toHaveLength(5);
      results.forEach((result, index) => {
        expect(result.response.content).toContain(`Concurrent request ${index + 1}`);
      });
    });

    it('should maintain consistent performance', async () => {
      const request = {
        userRequest: 'Performance test'
      };

      const times: number[] = [];

      for (let i = 0; i < 3; i++) {
        const startTime = Date.now();
        await orchestrator.processStructuredRequest(request);
        const endTime = Date.now();
        times.push(endTime - startTime);
      }

      // Performance should be consistent (within reasonable variance)
      const maxTime = Math.max(...times);
      const minTime = Math.min(...times);
      const variance = maxTime - minTime;

      expect(variance).toBeLessThan(100); // Less than 100ms variance
    });
  });

  describe('provider interface compliance', () => {
    it('should work with any LLMProvider implementation', async () => {
      class CustomProvider implements LLMProvider {
        getProviderName(): string {
          return 'custom-ai-provider';
        }

        async generateResponse(request: LLMRequest): Promise<LLMResponse> {
          return {
            content: `Custom AI response: ${request.userRequest.toUpperCase()}`,
            tokensUsed: {
              promptTokens: 75,
              completionTokens: 25,
              totalTokens: 100
            },
            model: 'custom-model-v1',
            finishReason: 'stop'
          };
        }

        async getSystemStatus() {
          return {
            isHealthy: true,
            rateLimitStatus: { canProceed: true },
            tokenBudgetStatus: { canProceed: true }
          };
        }

        destroy(): void {}
      }

      const customOrchestrator = new LLMOrchestrator(new CustomProvider());
      
      const request = {
        userRequest: 'test message'
      };

      const result = await customOrchestrator.processStructuredRequest(request);

      expect(result.response.content).toBe('Custom AI response: TEST MESSAGE');
      expect(result.tokenUsage.totalTokens).toBe(100);
      expect(customOrchestrator.getProviderName()).toBe('custom-ai-provider');
    });
  });
});
